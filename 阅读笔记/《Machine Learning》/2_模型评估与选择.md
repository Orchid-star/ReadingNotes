**错误率（error rate)**：分类错误的样本数a占样本总数m的比例，即*E = a / m*

**精度（accuracy）**：即正确率，与错误率之和为1

**误差（error）**：学习器的实际预测输出与样本的真实输出之间的差异

**训练误差（training error)、经验误差（empirical）**：学习器在训练集上的误差。实际能做的就是减小训练误差。

**泛化误差（generalization error)**：在新样本上的误差。

**过拟合（overfitting）**：学习器把训练样本学得太好，把自身的一些特点当作了所有潜在样本都具有的一般性质，导致泛化性能下降的现象。

**欠拟合（underfitting)**：学习器对训练样本一般性质没有学好的现象。

**模型选择（model selection)**：选择哪一种算法、如何设定算法的参数。理想方案是选择泛化误差最小的候选模型，但泛化误差无法直接获得，训练误差又由于过拟合现象的存在不适合作为标准。

**模型评估**：评估模型的好坏

**评估方法**：将测试集（testing set)的测试误差（testing error）作为泛化误差的近似。测试样本尽量不在训练集中出现、未在训练过程中使用过（训练与考试不能出相同的题是一个道理）。

**数据集产生训练集S与测试集T的方法**：

- 留出法（hold out) 将数据集D划分为两个互斥的集合，分别作为训练集S和测试集T。训练集和测试集要尽可能保持数据分布的一致性，通常采用分层采样（stratified sampling)  （3/4 - 4/5）
- 交叉验证法（cross validation）  k折交叉验证（k-fold cross validation），留一法。
- 自助法（bootstrapping)

**参数调节、调参（parameter tuning）**：对算法参数进行设定

**验证集（validation set）**：模型评估与选择中用于评估测试的数据集。实际中，把训练数据再划分为训练集和验证集，基于验证集上的性能进行模型选择和调参。

**性能度量（performance measure)**：衡量模型泛化能力的评价标准。如均方误差（mean squared error）。

**错误率、精度**：分类任务最常用的两种性能度量。

**差错率、查全率与*F1***

